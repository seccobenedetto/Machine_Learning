{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 2: Perceptrons\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement **MODEL SELECTION AND VALIDATION**.\n",
    "\n",
    "Complete all the **required code sections** and **answer all the questions**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    # TODO: Transform the input into homogeneous coordinates\n",
    "    return Xh_training, Xh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deterministic perceptron\n",
    "\n",
    "Now **complete** the function *perceptron*. <br>\n",
    "The **perceptron** algorithm **does not terminate** if the **data** is not **linearly separable**, therefore your implementation should **terminate** if it **reached the termination** condition seen in class **or** if a **maximum number of iterations** have already been run, where one **iteration** corresponds to **one update of the perceptron weights**. In case the **termination** is reached **because** the **maximum** number of **iterations** have been completed, the implementation should **return the best model** seen throughout.\n",
    "\n",
    "The current version of the perceptron is **deterministic**: we use a fixed rule to decide which sample should be considered (e.g., the one with the lowest index).\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the perceptron\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model (or the latest, if the termination condition is reached)\n",
    "- $best\\_error$: the *fraction* of misclassified samples for the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_errors(current_w, X, Y):\n",
    "    # This function:\n",
    "    # -computes the number of misclassified samples\n",
    "    # -returns the indexes of all misclassified samples\n",
    "    # -if there are no misclassified samples, returns -1 as index\n",
    "    # TODO: write the function\n",
    "    return n, index\n",
    "\n",
    "    \n",
    "def perceptron_fixed_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "    return new_w\n",
    "\n",
    "def perceptron_no_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation above of the perceptron to learn a model from the training data using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_found, error = perceptron_no_randomization(X_training,Y_training, 30)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "w_found2, error2 = perceptron_no_randomization(X_training,Y_training, 100)\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now use the best model $w\\_found$ to **predict the labels for the test dataset** and print the fraction of misclassified samples in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_estimate(w,X,Y):\n",
    "    # Estimate the test loss\n",
    "    return t_loss_estimate\n",
    "\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "    \n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomized perceptron\n",
    "\n",
    "Implement the correct randomized version of the perceptron such that at each iteration the algorithm picks a random misclassified sample and updates the weights using that sample. The functions will be very similar, except for some minor details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_randomized_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "    return new_w\n",
    "\n",
    "def perceptron_with_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test the correct version of the perceptron using 30 iterations and print the error of the best model we have found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the perceptron for 30 iterations\n",
    "w_found, error = perceptron_with_randomization(X_training, Y_training, 30)\n",
    "w_found2, error2 = perceptron_with_randomization(X_training, Y_training, 100)\n",
    "print(\"Training Error of perceptron (30 iterations): \" + str(error))\n",
    "print(\"Training Error of perceptron (100 iterations): \" + str(error2))\n",
    "\n",
    "true_loss_estimate =  loss_estimate(w_found, X_test, Y_test)       # Error rate on the test set\n",
    "true_loss_estimate2 =  loss_estimate(w_found2, X_test, Y_test) \n",
    "\n",
    "print(\"Test Error of perceptron (30 iterations): \" + str(true_loss_estimate))\n",
    "print(\"Test Error of perceptron (100 iterations): \" + str(true_loss_estimate2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Plot the loss with respect to the number of iterations (up to 1000)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
