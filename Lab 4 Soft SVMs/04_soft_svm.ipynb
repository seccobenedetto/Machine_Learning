{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning LAB 4: Soft SVM\n",
    "\n",
    "Course 2024/25: *F. Chiariotti*\n",
    "\n",
    "The notebook contains a simple learning task over which we will implement a **SOFT SUPPORT VECTOR MACHINE**.\n",
    "\n",
    "Complete all the **required code sections**.\n",
    "\n",
    "### IMPORTANT for the exam:\n",
    "\n",
    "The functions you might be required to implement in the exam will have the same signature and parameters as the ones in the labs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification of Stayed/Churned Customers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Customer Churn table contains information on all 3,758 customers from a Telecommunications company in California in Q2 2022. Companies are naturally interested in churn, i.e., in which users are likely to switch to another company soon to get a better deal, and which are more loyal customers.\n",
    "\n",
    "The dataset contains three features:\n",
    "- **Tenure in Months**: Number of months the customer has stayed with the company\n",
    "- **Monthly Charge**: The amount charged to the customer monthly\n",
    "- **Age**: Customer's age\n",
    "\n",
    "The aim of the task is to predict if a customer will churn or not based on the three features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the necessary Python libraries and load the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Dataset\n",
    "The dataset is a `.csv` file containing three input features and a label. Here is an example of the first 4 rows of the dataset: \n",
    "\n",
    "<center>\n",
    "\n",
    "Tenure in Months | Monthly Charge | Age | Customer Status |\n",
    "| -----------------| ---------------|-----|-----------------|\n",
    "| 9 | 65.6 | 37 | 0 |\n",
    "| 9 | -4.0 | 46 | 0 |\n",
    "| 4 | 73.9 | 50 | 1 |\n",
    "| ... | ... | ... | ... |\n",
    "\n",
    "</center>\n",
    "\n",
    "Customer Status is 0 if the customer has stayed with the company and 1 if the customer has churned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random as rnd\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn import linear_model, preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def load_dataset(filename):\n",
    "    data_train = pd.read_csv(filename)\n",
    "    #permute the data\n",
    "    data_train = data_train.sample(frac=1).reset_index(drop=True) # shuffle the data\n",
    "    X = data_train.iloc[:, 0:3].values # Get first two columns as the input\n",
    "    Y = data_train.iloc[:, 3].values # Get the third column as the label\n",
    "    Y = 2*Y-1 # Make sure labels are -1 or 1 (0 --> -1, 1 --> 1)\n",
    "    return X,Y\n",
    "\n",
    "# Load the dataset\n",
    "X, Y = load_dataset('data/telecom_customer_churn_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to differentiate (classify) between **class \"1\" (churned)** and **class \"-1\" (stayed)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Divide the data into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the splits\n",
    "m_training = int(0.75*X.shape[0])\n",
    "\n",
    "# m_test is the number of samples in the test set (total-training)\n",
    "m_test =  X.shape[0] - m_training\n",
    "X_training =  X[:m_training]\n",
    "Y_training =  Y[:m_training]\n",
    "X_test =   X[m_training:]\n",
    "Y_test =  Y[m_training:]\n",
    "\n",
    "print(\"Number of samples in the train set:\", X_training.shape[0])\n",
    "print(\"Number of samples in the test set:\", X_test.shape[0])\n",
    "print(\"Number of churned users in test:\", np.sum(Y_test==-1))\n",
    "print(\"Number of loyal users in test:\", np.sum(Y_test==1))\n",
    "\n",
    "# Standardize the input matrix\n",
    "# The transformation is computed on training data and then used on all the 3 sets\n",
    "scaler = preprocessing.StandardScaler().fit(X_training) \n",
    "\n",
    "np.set_printoptions(suppress=True) # sets to zero floating point numbers < min_float_eps\n",
    "X_training =  scaler.transform(X_training)\n",
    "print (\"Mean of the training input data:\", X_training.mean(axis=0))\n",
    "print (\"Std of the training input data:\",X_training.std(axis=0))\n",
    "\n",
    "X_test =  scaler.transform(X_test)\n",
    "print (\"Mean of the test input data:\", X_test.mean(axis=0))\n",
    "print (\"Std of the test input data:\", X_test.std(axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use **homogeneous coordinates** to describe all the coefficients of the model.\n",
    "\n",
    "_Hint:_ The conversion can be performed with the function $hstack$ in $numpy$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_homogeneous(X_training, X_test):\n",
    "    Xh_training = np.hstack([np.ones( (X_training.shape[0], 1) ), X_training])\n",
    "    Xh_test = np.hstack([np.ones( (X_test.shape[0], 1) ), X_test])\n",
    "    return Xh_training, Xh_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert to homogeneous coordinates using the function above\n",
    "X_training, X_test = to_homogeneous(X_training, X_test)\n",
    "print(\"Training set in homogeneous coordinates:\")\n",
    "print(X_training[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Soft SVM with stochastic gradient descent\n",
    "\n",
    "Now **complete** the function *sgd_soft_svm* and all auxiliary functions. <br>\n",
    "You should select *a single sample*, compute the gradient, and run the soft SVM version (.\n",
    "\n",
    "The input parameters to pass are:\n",
    "- $X$: the matrix of input features, one row for each sample\n",
    "- $Y$: the vector of labels for the input features matrix X\n",
    "- $max\\_num\\_iterations$: the maximum number of iterations for running the soft SVM\n",
    "- $averaging\\_iterations$: the number of iterations to consider when averaging\n",
    "\n",
    "The output values are:\n",
    "- $best\\_w$: the vector with the coefficients of the best model\n",
    "- $margin$: the *margin* of the best model\n",
    "- $outliers$: the number of outliers that are classified correctly by the best model\n",
    "- $misclassified$: the number of outliers that are misclassified by the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_outliers(current_w, X, Y):\n",
    "    # Return a tuple containing 2 numbers:\n",
    "    # First, the number of total outliers (distance below the margin)\n",
    "    # Second, the number of misclassified outliers\n",
    "    \n",
    "def find_margin(current_w):  \n",
    "    # Return the margin for the selected model\n",
    "\n",
    "def sgd_soft_svm(X, Y, lambda_par, max_num_iterations, averaging_iterations):\n",
    "    # Initialize the weights of the algorith with w=0\n",
    "    theta = np.zeros(X.shape[1])\n",
    "    best_w = np.zeros(X.shape[1])\n",
    "    num_samples = X.shape[0]\n",
    "\n",
    "    # Loop the SGD algorithm\n",
    "    for num_iter in range(max_num_iterations):\n",
    "        # Compute the current weights\n",
    "        # Compute the gradient over a random point\n",
    "        if (num_iter >= max_num_iterations - averaging_iterations):\n",
    "            # Use the current model for averaging\n",
    "\n",
    "    return best_w, margin, outliers, misclassified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use the implementation to learn a model from the training data using 100000 iterations and averaging over the last 10000. Let us consider $\\lambda=1$. Then we use the best model $best\\_w$ to **predict the labels for the test dataset** and print the fraction of outliers in the test set (the test error that is an estimate of the true loss)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the Soft SVM with lambda=0.1. Consider 100000 iterations, 10000 of which are avera\n",
    "best_w, margin, outliers, misclassified = sgd_soft_svm(X_training, Y_training, 0.1, int(1e5), int(1e4))\n",
    "print(\"Soft SVM model: \" + str(best_w))\n",
    "print(\"Soft SVM margin: \" + str(margin))\n",
    "print(\"Total outliers: \" + str(outliers))\n",
    "print(\"Misclassified points: \" + str(misclassified))\n",
    "true_error = np.asarray(count_outliers(best_w, X_test, Y_test)) / len(Y_test)\n",
    "\n",
    "print(\"Total outlier fraction (test set): \" + str(true_error[0]))\n",
    "print(\"True loss (test set): \" + str(true_error[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us compare the result with your perceptron function from Lab 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perceptron_randomized_update(current_w, X, Y):\n",
    "    # TODO: write the perceptron update function\n",
    "\n",
    "def count_errors(current_w, X, Y):\n",
    "    # Find all indices which have a different sign from the corresponding labels\n",
    "    index = np.nonzero(np.sign(np.dot(X, current_w)) - Y)[0]\n",
    "    n = np.array(index).shape[0]\n",
    "    if (n == 0):\n",
    "        # There are no misclassified samples\n",
    "        return 0, -1\n",
    "    return n, index\n",
    "\n",
    "def perceptron_with_randomization(X, Y, max_num_iterations):\n",
    "    # TODO: write the perceptron main loop\n",
    "    # The perceptron should run for up to max_num_iterations, or stop if it finds a solution with ERM=0\n",
    "    return best_w, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us test the error of the Soft SVM against the perceptron's best model, using 1000 iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_found, error = perceptron_with_randomization(X_training, Y_training, 1000)\n",
    "print(\"Training Error of perceptron: \" + str(error))\n",
    "print(\"Best perceptron model: \" + str(w_found))\n",
    "true_loss_estimate =  count_errors(w_found, X_test, Y_test)[0] / len(Y_test)    # Error rate on the test set    \n",
    "print(\"Test Error of perceptron: \" + str(true_loss_estimate))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can try to see the effect of $\\lambda$. Consider values $10, 1, 0.1, 0.01, 0.001$ and run a K-fold cross validation (you can use the code from Lab 3). Plot the margin and outlier count. Use the loss (i.e., the number of misclassified points) as a score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_fold(X_training: np.ndarray, Y_training: np.ndarray, lambda_vec: np.ndarray, K: np.ndarray) -> None:\n",
    "    ## TODO: perform cross-validation\n",
    "    # Divide training set in K folds\n",
    "    fold_points = int(np.floor(max_idx / K))\n",
    "    for lambda_idx in range(len(lambda_vec)):\n",
    "        lambda_perf = 0\n",
    "        for test in range(K):\n",
    "            # Cross-validation step\n",
    "        lambda_perf /= K\n",
    "        results.append(lambda_perf)\n",
    "        models.append(sgd_soft_svm(X_shuffled, Y_shuffled, lambda_par[lambda_idx], int(1e5), int(1e4)))\n",
    "        if (best_perf > lambda_perf):\n",
    "            # Improvement on the model\n",
    "    return best, best_perf, models, results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the training with K-fold cross-validation and plot the score\n",
    "K = 5\n",
    "lambda_par = [10, 1, 1e-1, 1e-2, 1e-3]\n",
    "\n",
    "best_model, best_perf, models, results = K_fold(X_training, Y_training, lambda_par, K)\n",
    "print(best_model, results)\n",
    "plt.plot(np.log10(lambda_par), results)\n",
    "plt.xlabel('$\\log(\\lambda)$')\n",
    "plt.ylabel('Validation loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What does this tell you about the margins? Which one should you choose? Now compute the test loss of the best Soft SVM. What does this tell you about the algorithm choice over this problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_error = np.asarray(count_outliers(best_model[0], X_test, Y_test)) / len(Y_test)\n",
    "print(\"Total outlier fraction (test set): \" + str(true_error[0]))\n",
    "print(\"True loss (test set): \" + str(true_error[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
